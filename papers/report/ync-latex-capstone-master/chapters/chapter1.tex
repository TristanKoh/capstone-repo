% Chapter 1

\chapter{Introduction} % Main chapter title

\label{chapter1} % For referencing the chapter elsewhere, use \ref{Chapter1}

%----------------------------------------------------------------------------------------
\section{Motivation and significance}
A question was posed in 2017: "Will there be a software service for the generation of explanations and arguments in law that will assist in structuring explanations of answers and supportive legal arguments?" (\cite{ashley_2017}). This question seems close to getting an answer in 2023 with the release of ChatGPT (\cite{openai}) to the general public. Consider the following conversation with ChatGPT where I documented some of its legal writing abilities\footnote{The question prompts and the names of the specific writing techniques were adapted from \cite{bishop2023computer}.}: 
\begin{enumerate}
  \item Explaining technical concepts. 
  \begin{itemize}
    \item[Q:] Explain what the eggshell skull rule means in negligence law. 
    \item[A:] The "eggshell skull" rule is a principle in tort law that states that a defendant is liable for all damages suffered by a plaintiff, even if the plaintiff's injuries are unusually severe or unexpected. The rule means that the defendant must take the plaintiff as they find them, meaning that they cannot argue that the plaintiff's injuries would not have been so severe if they were a "normal" person.
  \end{itemize}
  \item Adjusting for level and tone. 
  \begin{itemize}
    \item[Q:] Explain the eggshell skull rule like I'm five.
    \item[A:] The "eggshell skull" rule means that if someone hurts another person, they are responsible for all the harm they cause, even if the person they hurt is more easily hurt than most people. It's like if you accidentally hit someone on the head and they have a very fragile skull, you would be responsible for all the damage, even if it's worse than if you hit someone with a normal skull. So you have to be extra careful not to hurt someone, even if they seem more delicate or breakable than most people.
  \end{itemize}
  \item Providing a legal opinion. 
  \begin{itemize}
    \item[Q:] I think that the eggshell skull rule is unfair. Can you make that sound academic?
    \item[A:] Certainly. One could argue that the eggshell skull rule is inequitable, as it places an undue burden on defendants to anticipate and accommodate a plaintiff's pre-existing condition or vulnerability. By imposing liability on defendants for the full extent of the harm caused, regardless of the extent to which the harm would have been suffered by a normal person, the rule ignores the fact that the plaintiff's fragile condition is not the fault of the defendant. Additionally, the rule may incentivize plaintiffs to conceal pre-existing conditions, thereby undermining the efficiency of the tort system.
  \end{itemize} 
\end{enumerate}

In 2017, some commentators opined that AI would be unlikely to take over higher-value roles of the lawyer such as advising clients, writing legal briefs, negotiating and appearing in court (\cite{legaltech_nytimes}). However, as seen from the above, ChatGPT seems to have the ability to automate (or at least assist) these areas of legal practice, except perhaps negotiation and court appearances since ChatGPT does not have a physical presence. Indeed, most recently, ChatGPT has been said to have "the markings of an advocate" (\cite{legaltech_reuters}).

ChatGPT is the most recent AI model that uses advanced natural language processing (NLP). Natural language forms the bread and butter of the legal industry, as it is expressed in contracts, judgements and legislation. The legal industry has been adopting more machine learning tools to automate and assist low level legal analysis. Worldwide legal tech market revenues were at 27.6 billion USD and is projected to to grow at a compound annual growth rate of 4\% to 35.6 billion USD by 2027 (\cite{statista}). As early as 2018, LawGeex, a contract review startup, compared the performance of lawyers vs LawGeex's machine learning model in reviewing standard template Non-Disclosure Agreements (NDA). The model beat the humans both in terms of accuracy and time, with the model having a 94\% accuracy rate and taking 26 seconds to complete the review. In comparison, the lawyers had an average accuracy of 85\% and took 92 minutes to finish the task (\cite{lawgeex}). More significantly, at the end of 2022, OpenAI's AI model GPT (Generative Pre-trained Transformer)\footnote{The GPT is also used for ChatGPT.} took the US bar examination and got 50\% of the questions correct, and performed at a passing rate for both Evidence and Torts (\cite{bommarito2022}).

Like the models that were trained by LawGeex and OpenAI, most legal tech tools that conduct low level legal analysis use natural language processing (NLP) techniques. NLP is a branch of AI that gives computers the ability to understand text and spoken words as how human beings would understand (\cite{ibm_nlp}). While NLP techniques have substantially increased in performance in recent years, it has come at the cost of the explainability of their predictions because of models that are architecturally more complex (\cite{zini2022}). This issue of explainability is exemplified by the last question I posed to ChatGPT about its view on the egg-shell skull rule:

\begin{itemize}
  \item[Q:] How did you think of that answer?
  \item[A:] As an AI language model, I was programmed to provide responses based on a given prompt. In this case, I generated a response to your prompt based on my training data and my understanding of the English language.
\end{itemize}

ChatGPT does not seem to be able to explain its views like how a typical human would\footnote{Further prompting led ChatGPT to provide a list of academic papers that support its view.}. This lack of explainability could potentially be a significant hindrance towards NLP's further adoption within the legal industry because the lawyer and the law firm which use these models still ultimately bear the legal responsibility of ensuring that the analysis is legally sound. For example, r5(2)(c) of the Legal Profession (Professional Conduct) Rules 2015 states that a legal practitioner must act with reasonable diligence and competence in the provision of services to the client. A lawyer that relies on the analysis of legal tech tools and does not understand how the analysis was produced could be considered as lacking in diligence and competence.

Nevertheless, the intersection in skillset between data science and legal analysis is still nascent and it is unrealistic to expect all legally trained personnel to be trained in data science to the extent required to interpret the predictions of machine learning models without aid. At the same time, explainable AI (XAI) techniques and research have been rising in popularity since 2020 (\cite{linardatos2020}) but have not been specifically applied onto legal text. Therefore, this capstone aims to bridge the gap between the lawyer and the data scientist by using XAI techniques to explain the predictions of machine learning models. 

Separately, the widespread collection and use of data by organisations in recent years has led to an increase of regulations governing data privacy. This "datafication" of society includes the "transformation of interactions into data that can be valued and used for predictive analysis". Governments have therefore stepped up their efforts to guarantee privacy, with 145 countries having enacted data protection legislation in 2021 (\cite{gstrein2022}). With more sophisticated regulation comes increased difficulties for organisations to ensure that they are complying with these regulations, and for governments to enforce them. A possible area of legal tech would be tools to aid in the compliance of these regulations. Therefore, I focus on NLP and XAI in the specific context of data privacy. This context provides a realistic evaluation of the interpretability of models that are trained on legal texts relating to data privacy\footnote{All code and analysis used in this capstone can be found at \url{https://github.com/TristanKoh/capstone-repo/}.}.

\subsection{The increasing opacity of NLP and the rise of XAI}
\label{chap1:increasing_opacity}
In NLP, there are inherent difficulties in translating natural language to a mathematical representation that can be understood by the computer. Consider the sentence: "I am cow." Using a Bag-of-Words approach which represents words in terms of their frequency in the sentence, the word "cat" can be represented as:

\begin{lstlisting}
  cow = [0, 0, 1]
\end{lstlisting}

Therefore every word in this sentence can be encoded as: 

\begin{lstlisting}
  I = [1, 0, 0]
  am = [0, 1, 0]
  cow = [0, 0, 1]
\end{lstlisting}

Each word can then be represented as a point with $x, y, z$ coordinates in a three-dimensional space. The plane that passes through all three points would be a graphical representation of the sentence "I am cow". Mathematically, the equation $x + y + z - 1 = 0$ describes the sentence. Clearly there is a big difference in abstraction between the original sentence and the mathematical representation of the same sentence! Without knowing the process of how this equation was derived from the sentence, this equation could possibly refer to any other 3D object, such as the surface of a table.

Further, the Bag-of-Words approach only represents the sentence by the number of occurrences of each word in the sentence. The semantic difference between "I am cat" and "I am cow" (both are four legged animals that are mammals) vs "I am book" and "I am cow" (a cow is a living thing while a book is non-living) is not captured at all by the Bag-of-Words representation\footnote{Furthermore, these two sentences cannot be represented graphically since it introduces a fourth dimension.}. To the computer, "I am cat" and "I am book" are equally different from "I am cow" since both sentences respectively contain an instance of "cat" and "book" which "I am cow" does not contain. In fact, the sentence "I am cow cow" would make sense to the computer even though it makes no sense semantically.

Evidently, even the simplest word representation in NLP poses significant limitations in capturing how humans understand language. Word representations have since progressed tremendously in capturing more semantic meaning, as exemplified by GPT being able to pass certain subjects in the US bar exam. However, to capture such semantic meaning requires more abstraction and further increases the opacity and decreases the interpretability of NLP models. Hence, there is an inverse relationship between performance / opacity and interpretability.  This is typically described as the "black-box" problem of AI: only the inputs and outputs to the system can be observed, but how the model derived the outputs from the inputs is not known (or at least not easily understood) because it is difficult to know exactly how the model is programmed (\cite{zednik2021}).

Therefore, XAI for NLP has been designed to combat such issues. Explainability in this context is seen "from the perspective of an end user whose goal is to understand how a model arrives at its result" (\cite{danilevsky2020}). XAI aims to reduce the abstraction and opacity of NLP techniques so that end users can understand how the model arrived at a decision, which makes the "black-box" more of a "glass-box". However, there is not much consensus about what "explainable" means. While the general agreed upon goal of XAI is to "completely, accurately and clearly quantify the [model's] logic", there is no consistent use of the terms "explainability, interpretability and transparency". Interpretability is sometimes used to describe the model's internal logic and explainability as the ability of the user to understand that logic. In other cases, explainability is the attempt to explain the model's logic post-hoc without necessarily being representative of the model's true decision (\cite{rosenfeld2021}). Since arguing what should be the "correct" definition of explainability is out of the scope of this capstone, I use explainability and interpretability to refer to the general goal of XAI as stated above.

Another growing area of discourse is differentiating what explainability means to different users of the model who have different objectives for the explanations. For example, what could be explainable to data scientists may not be explainable to laypersons. While data scientists may find that providing more technical details about the model would make the model more explainable, laypersons might be more confused if too many details are provided. Further, if a person wants the model to explain why it recommended a certain product over another to purchase on Amazon, the type and level of detail of explanation would be very different compared to a situation where the model predicts that the accused has a higher risk of recidivism which therefore warrants a longer prison term (\cite{rosenfeld2021}). Therefore, assessing the effectiveness of XAI is highly dependent on the specific context and needs of the users. As will be explained in the following section, I focus specifically on what explainability means to three different stakeholders, the consumer, organisation and state for the purposes of analysing apps' data privacy policies.

\subsection{The importance of XAI in data privacy regulation}
As with any regulatory activity, there are three stakeholders: The consumer, the organisation and the state. The importance of XAI in data privacy can also be analysed from the perspectives of these three stakeholders.

\subsubsection{The consumer}
It is uncontroversial that data privacy policies on websites and software are rarely read, and even if they are read, consumers are unlikely to fully understand them because of the use of extensive legalese. Using NLP, Wagner conducted a text analysis of privacy policies from 1996 - 2021, and made the following findings (\cite{wagner2022privacy}): 

\begin{enumerate}
  \item The length of the average policy has doubled in the last 10 years, with the introduction of the General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA) leading to significant increases in length.
  \item In 2021, the average Flesch Reading Ease of the policies is roughly the same as compared to academic articles such as the Harvard Law Review.
  \item In 2021, the average policy takes 17 minutes to read, and the annual reading time per consumer is more than 400 hours which is more than an hour per day\footnote{Assuming that a consumer visits 1462 unique websites each year.}.
  \item Policies have including more vague wording and invasive data privacy practices, such as implicit collection of location data, no specification as to whether data is anonymised, no meaningful choice given to consumers when there is a change of policy, and generic wording about security measures used to protect data.
\end{enumerate}

The increasing unreadability of data privacy policies poses a significant challenge to the effectiveness of data privacy regulation given that the current model of regulation depends on the consumer giving consent that is informed and freely given (\cite{mantelero_2014})\footnote{Mantelero also discusses a different model of data regulation to combat the issues of the current informed consent paradigm, but that is out of the scope for this capstone.}. Combined with the datafication of society, consumers have diminishing control over how their data is collected and used. 

In view of the unreadability of data privacy policies, legal tech that leverages on XAI can be useful to reduce this unreadability. Legal tech able to analyse and explain the implications of data privacy policies in simple terms to the consumer would greatly aid making these policies more accessible to the consumer, help in allowing them to make informed consent, and saves them time and money as compared to seeking legal advice.

\subsubsection{The organisation}
Though there are many data privacy regulations globally, I focus my analysis on the EU and Singapore since EU's GDPR has the widest scope of any data privacy regulation. I also chose Singapore as this author is based in Singapore. The GDPR is applicable to any personal data processing linked to the EU, either when the entity processing the personal data is established within the EU or when an entity outside the EU offers goods and services to people within the EU (\cite{gdpr_art3}). Therefore, the GDPR is not limited to EU's geographic boundaries and can potentially apply to any entity in any country. Organisations in the EU are subjected to a data subject's "right to explanation". Under Recital 71, data subjects have the right to obtain an explanation of a decision reached solely through automated processing (\cite{gdpr_recital71}). This right could be said to be supported by Art. 22, where data subjects have the right not to be subject to a decision based solely on automated processing (\cite{gdpr_art22}). For example, a bank could use AI to predict the probability of a customer defaulting on a loan. This prediction could be used to justify a decision to deny a loan to the customer. Under the GDPR, the customer has the right to not be subjected to such automated processing and obtain an explanation as to why the model made such a prediction.

In Singapore, explanability of AI decision is found in the non-binding Model AI Governance Framework released by the Personal Data Protection Commission (PDPC) (\cite{ai_modelframework}). While this framework is not specifically about data privacy, AI governance is closely related to data protection as the development of AI models is dependent on how personal data is governed. As part of guidance on the operations management of AI models, organisations are advised to provide explanations on how AI models are incorporated into the decision making process of the organisations so as to build understanding and trust with those stakeholders that use their products. Explainability also forms part of stakeholder communication. In terms of communications with their stakeholders, organisations are advised to develop policies on the type of explanations and when to provide them. Such communication could include explanations as to how the AI model was used in a specific decision. In this regard, the Governance Framework also encompasses the GDPR's right to explanation.

Though there has been debate as to the scope of the GDPR's right to explanation, whether it is binding, and how it is balanced against other interests protected by the GDPR (\cite{chesterman2021_transparency}), such legislation in addition to Singapore's AI Governance Framework that create guidance with regard to the explanability of automated decision making supports the need for further development of XAI specifically in the context of data privacy. The models in this capstone can support the adoption of AI in organisations by providing explanations so as to aid the organisations in checking whether their privacy policies are in compliance with data privacy regulations.

\subsubsection{The state}
At a more macro level, the state's concern when AI is used in legal decision making is maintaining the integrity of the judicial system and regulatory activities. Three challenges arise because of opacity when AI is used in legal decision making in a data privacy context (\cite{chesterman2021_opacity}): 

\begin{enumerate}
  \item Inferior decision making as opacity reduces opportunities to identify wrongdoing or areas of reasoning which can be improved. 
  
  For example, TikTok's algorithm that recommends users' videos on their feed collects and aggregates user data to predict their interests. This algorithm is known to be much more aggressive compared to other social networks and has played a crucial role in its recent spike in popularity (\cite{tiktok_guardian}). It optimises for time spent viewing each video (including the time that the user returns to the same video) to recommend more videos that aim to keep the user scrolling on the app. While TikTok has disclosed the broad factors that the algorithm uses, the actual algorithm is much more nuanced as seen at how an internal company document describes recommending more "sad" content to increase watch time which could lead to videos involving self-harm (\cite{tiktok_nytimes}). With opacity in the algorithm, users (and regulators) do not know how their personal data is being used to recommend them videos. This could lead to adverse social consequences when the algorithm recommends controversial content such as religiously or politically partisan posts. Opacity also prevents regulators from pre-empting or intervening in such undesirable social consequences such as mandating that such recommendation algorithms must have limits to the type of content that can be recommended.
  
  \item Impermissible decisions such as reifying discrimination as opacity masks such decisions with the "objective" nature of AI predictions.
  
  A common example of this is using AI to predict which bank customers would default on their loans. AI models are trained to predict the risk of default based on demographic metrics, such as employment status and income level. If the model assesses the customer is likely to default, the bank would probably not allow the customer to take out a loan. In some jurisdictions, anti-discrimination legislation prevents banks from making such assessments with regard to race, gender, colour, religion etc. which are considered personal data under data privacy regulation\footnote{Such as the Fair Housing Act in the US that makes it unlawful for any lender to discriminate against any person because of race, colour, religion, national origin, sex, handicap or familial status (\cite{fairhousingact}).}. If opaque AI models are used, it would not be possible to ascertain whether these protected personal data are actually used for the prediction. It could be possible to justify a decision not to grant a loan just because the model made a prediction that the customer is likely to default. This prediction could be seen as "objective" as it was made by an algorithm rather than a human even though it actually perpetuates racism because it used race as a predictor (for example). Hence, opacity in AI models potentially pose societal and regulatory issues.

  \item Legitimacy of decisions since the legal system depends as much on the justification of decisions as it does on the decisions themselves.
  
  In the context of data privacy regulation in Singapore, Parliament enacts the PDPA while the PDPC is empowered to create regulations and enforce the PDPA\footnote{Per s65(1) of the PDPA.}. There is an appeal process for cases appearing before the PDPC and the higher courts can overturn the PDPC's decisions\footnote{Per s48Q, s48R and s54 of the PDPA.}. However, when overturning the PDPC's decisions, the higher courts do not disagree with the outcome but they disagree with the reasoning of the PDPC's decision. Assuming that AI models are sophisticated enough to write court judgements, if there was no explanation of the model's judgement, there would be no basis for the higher courts to overturn judgements made by lower courts. 
  
  Further, in common law systems, judges' decisions become part of case law and past decisions become binding on new cases with the same facts (i.e. the doctrine of \textit{stare decisis}). One of the benefits of case law is that it can be developed incrementally and adapted to suit real facts unlike legislation, so that the law actually meets the needs of the parties. While \textit{stare decisis} does not apply to PDPC cases at first instance, the same principle of incrementally developing and applying the law still applies. For example, the quantum of fine imposed on organisations in breach of the PDPA is determined according to how severe the breach is in comparison with similar past cases\footnote{An example would be \textit{RedMart Limited} [2022] SGPDPC 8 at [33].}. If opaque AI is used to write or aid in the writing of these decisions, there would be fewer opportunities to improve the law since the reasoning would likely to be insufficiently explicit. Hence the development of the law would also be impeded.
\end{enumerate}

Hence, the importance of XAI applies across all three stakeholders in data privacy. Consumers can benefit from XAI by making privacy policies more readable. Organisations can use XAI to be more accountable to consumers and regulators by providing explanations for automated decisions. XAI also provides the state with more transparency in regulating organisations' use of AI and aiding in judicial decision making.

\section{Problem statement}
The foregoing analysis of the issues in data privacy and the opacity of NLP models can be summarised as follows: NLP techniques are now much more advanced and can potentially automate some areas of substantive legal reasoning and writing, but has come at the cost of increased opacity of the models. More research has been put into XAI in an effort to combat this opacity, but it is an open question as to whether the explanations are effective to the different users and use cases of the models. Data privacy regulation can benefit from the use of more XAI in automated decision making. Thus, there is a gap in research where XAI is assessed and applied in data privacy. 

Hence, I choose to train AI models that are able to identify data privacy practices in apps' data privacy policies. A privacy practice describes a certain behaviour of an app that can have privacy implications\footnote{This will be further elaborated on when \hyperref[app350_corpus]{the dataset} is introduced.}. In this regard, these models function similarly to that of LawGeex's contract review models which are able to identify the type of clause, such as which party owns the intellectual property arising from the performance of the contract\footnote{Automated contract review models are also able to analyse the level of legal risk associated with any amendments made to the clause, such as if the counterparty amended the wording of the clause to state that only the counterparty owns the intellectual property instead of both contracting parties.}. XAI methods are then used to visualise why the models made such predictions. Such a legal tech tool saves consumers time by highlighting specific data privacy practices upfront so that they can make informed consent. 

I focus on these specific problems for the rest of the capstone:

\begin{enumerate}
  \item Which AI models perform the best on these privacy policy texts and why?
  \item Which models are the most explainable to an audience that includes laypersons and people with domain knowledge in data science and the law, and why?
  \item Are there differences in how people perceive explainability if they were asked to consider the predictions of these models from the perspective of a consumer, an organisation or the PDPC (as the regulatory authority for data privacy), and why?
\end{enumerate}

\section{Main findings and roadmap}
Chapter~\ref{chapter2} is a literature review of XAI methods of NLP and methods of assessing these XAI methods which forms the justification of the methodology and dataset used. Chapter~\ref{chapter3} presents an exploratory data analysis of the dataset. Chapter~\ref{chapter4} reports the performance of the models that were trained on the dataset. 

\section{Font Formatting Commands}
Similarly to Word, LaTeX provides simple formatting, including \textbf{bold}, \textit{italic}, \underline{underlined} and \texttt{ugly stuff}.
However, no underline or strikethrough by default.
You can also change the size of the text, using {\tiny tiny}, {\small small}, {\large large}, {\huge huge}.

\subsection{Special characters}
In that case, simply use \textdollar{} (by the way, note that using the dollar sign in your text switches to mathematical notation. To actually print a dollar sign use the \textbackslash{}textdollar command).
The equation above has a label, meaning you can refer to it. The numbering system uses the chapter number (in this case 1), then the equation position within the chapter (1 again).
Example: Equation~\ref{eq:eq1} is an example of an equation in LaTeX{}.
In case you would like to have an equation without numbering it? Easy!
\begin{equation*}
t = a \times log_{2}(\frac{D}{W} + 1) + b
\end{equation*}

The only difference? The \textasteriskcentered{}  symbol in the \textbackslash{}begin\{equation\textbf{\textasteriskcentered}\}.
This also works with Figures and Tables.