% Chapter 1

\chapter{Introduction} % Main chapter title

\label{chapter1} % For referencing the chapter elsewhere, use \ref{Chapter1}

%----------------------------------------------------------------------------------------
\section{Motivation and significance}
Natural language forms the bread and butter of the legal industry, as it is expressed in contracts, judgements and legislation. The legal industry has been adopting more machine learning tools to automate and assist low level legal analysis. Worldwide legal tech market revenues were at 27.6 billion USD and is projected to to grow at a compound annual growth rate of 4\% to 35.6 billion USD by 2027 (\cite{statista}). As early as 2018, LawGeex, a contract review startup, compared the performance of lawyers vs LawGeex's machine learning model in reviewing standard template Non-Disclosure Agreements (NDA). The model beat the humans both in terms of accuracy and time, with the model having a 94\% accuracy rate and taking 26 seconds to complete the review. In comparison, the lawyers had an average accuracy of 85\% and took 92 minutes to finish the task (\cite{lawgeex}). More significantly, at the end of 2022, OpenAI's AI model GPT (Generative Pre-trained Transformer)\footnote{The GPT forms the foundation of the now famous ChatGPT.} took the US bar examination and got 50\% of the questions correct, and performed at a passing rate for both Evidence and Torts (\cite{bommarito2022}).

Like the models that were trained by LawGeex and OpenAI, most legal tech tools that conduct low level legal analysis use natural language processing (NLP) techniques. NLP is a branch of AI that gives computers the ability to understand text and spoken words as how human beings would understand (\cite{ibm_nlp}). While NLP techniques have substantially increased in performance in recent years, it has come at the cost of the explainability of their predictions because of models that are architecturally more complex (\cite{zini2022}). This lack of explainability could potentially be a significant hindrance towards NLP's further adoption within the legal industry because the lawyer and the law firm which use these models still ultimately bear the legal responsibility of ensuring that the analysis is legally sound\footnote{For example, r5(2)(c) of the Legal Profession (Professional Conduct) Rules 2015 states that a legal practitioner must act with reasonable diligence and competence in the provision of services to the client. A lawyer that relies on the analysis of legal tech tools and does not understand how the analysis was produced could be considered as lacking in diligence and competence.}.

Nevertheless, the intersection in skillset between data science and legal analysis is still nascent and it is unrealistic to expect all legally trained personnel to be trained in data science to the extent required to interpret the predictions of machine learning models without aid. At the same time, explainable AI (XAI) techniques and research have been rising in popularity since 2020 (\cite{linardatos2020}) but have not been specifically applied onto legal text. Therefore, this capstone aims to bridge the gap between the lawyer and the data scientist by using XAI techniques to explain the predictions of machine learning models. 

Separately, the widespread collection and use of data by organisations in recent years has led to an increase of regulations governing data privacy. This "datafication" of society includes the "transformation of interactions into data that can be valued and used for predictive analysis". Governments have therefore stepped up their efforts to guarantee privacy, with 145 countries having enacted data protection legislation in 2021 (\cite{gstrein2022}). With more sophisticated regulation comes increased difficulties for organisations to ensure that they are complying with these regulations, and for governments to enforce them. A possible area of legal tech would be tools to aid in the compliance of these regulations. Therefore, I focus on NLP and XAI in the specific context of data privacy. This context provides a realistic evaluation of the interpretability of models that are trained on legal texts relating to data privacy\footnote{All code and analysis used in this capstone can be found at \url{https://github.com/TristanKoh/capstone-repo/}.}.

\subsection{The increasing opacity of NLP and the rise of XAI}
In NLP, there are inherent difficulties in translating natural language to a mathematical representation that can be understood by the computer. Consider the sentence: "I am cow." Using a Bag-of-Words approach which represents words in terms of their frequency in the sentence, the word "cat" can be represented as:

\begin{lstlisting}
  cow = [0, 0, 1]
\end{lstlisting}

Therefore every word in this sentence can be encoded as: 

\begin{lstlisting}
  I = [1, 0, 0]
  am = [0, 1, 0]
  cow = [0, 0, 1]
\end{lstlisting}

Each word can then be represented as a point with $x, y, z$ coordinates in a three-dimensional space. The plane that passes through all three points would be a graphical representation of the sentence "I am cow". Mathematically, the equation $x + y + z - 1 = 0$ describes the sentence. Clearly there is a big difference in abstraction between the original sentence and the mathematical representation of the same sentence! Without knowing the process of how this equation was derived from the sentence, this equation could possibly refer to any other 3D object, such as the surface of a table.

Further, the Bag-of-Words approach only represents the sentence by the number of occurrences of each word in the sentence. The semantic difference between "I am cat" and "I am cow" (both are four legged animals that are mammals) vs "I am book" and "I am cow" (a cow is a living thing while a book is non-living) is not captured at all by the Bag-of-Words representation\footnote{Furthermore, these two sentences cannot be represented graphically since it introduces a fourth dimension.}. To the computer, "I am cat" and "I am book" are equally different from "I am cow" since both sentences respectively contain an instance of "cat" and "book" which "I am cow" does not contain. In fact, the sentence "I am cow cow" would make sense to the computer even though it makes no sense semantically.

Evidently, even the simplest word representation in NLP poses significant limitations in capturing how humans understand language. Word representations have since progressed tremendously in capturing more semantic meaning, as exemplified by GPT being able to pass certain subjects in the US bar exam. However, to capture such semantic meaning requires more abstraction and further increases the opacity and decreases the interpretability of NLP models. Hence, there is an inverse relationship between performance / opacity and interpretability.  This is typically described as the "black-box" problem of AI: only the inputs and outputs to the system can be observed, but how the model derived the outputs from the inputs is not known (or at least not easily understood) because it is difficult to know exactly how the model is programmed (\cite{zednik2021}).

Therefore, XAI for NLP has been designed to combat such issues. Explainability in this context is seen "from the perspective of an end user whose goal is to understand how a model arrives at its result" (\cite{danilevsky2020}). Danilvesky et al. categorises XAI for NLP according to two categories: explainability techniques and visualisation techniques. Explainability techniques are ways to generate the raw mathematical justifications that led to the final explanation presented to the end users. For example, feature importance is one technique that identifies the most important words / phrases in the sentence that led to the prediction. In contrast, visualisation techniques are different ways to present these mathematical justifications to the end user. One example is a saliency heatmap, which highlights the combination of words / phrases that gave rise to the prediction at differing intensities\footnote{More details about these XAI techniques would be covered later in \autoref{chapter2}.}. (add figures here to show the differences) Overall, XAI aims to reduce the abstraction and opacity of NLP techniques so that end users can understand how the model arrived at a decision, which makes the "black-box" more of a "glass-box".

\subsection{The importance of XAI in legal technology}
The release of ChatGPT (\cite{openai}) to the general public at the start of 2023 probably has the potential of automating some areas of substantive legal writing and analysis. I documented the following (non-exhaustive) legal writing abilities of ChatGPT\footnote{The question prompts and the names of the specific writing techniques were adapted from \cite{bishop2023computer}.}: 
\begin{enumerate}
  \item Explaining technical concepts. 
  \begin{itemize}
    \item[Q:] Explain what the eggshell skull rule means in negligence law. 
    \item[A:] The "eggshell skull" rule is a principle in tort law that states that a defendant is liable for all damages suffered by a plaintiff, even if the plaintiff's injuries are unusually severe or unexpected. The rule means that the defendant must take the plaintiff as they find them, meaning that they cannot argue that the plaintiff's injuries would not have been so severe if they were a "normal" person.
  \end{itemize}
  \item Adjusting for level and tone. 
  \begin{itemize}
    \item[Q:] Explain the eggshell skull rule like I'm five.
    \item[A:] The "eggshell skull" rule means that if someone hurts another person, they are responsible for all the harm they cause, even if the person they hurt is more easily hurt than most people. It's like if you accidentally hit someone on the head and they have a very fragile skull, you would be responsible for all the damage, even if it's worse than if you hit someone with a normal skull. So you have to be extra careful not to hurt someone, even if they seem more delicate or breakable than most people.
  \end{itemize}
  \item Expanding on a particular view. 
  \begin{itemize}
    \item[Q:] I think that the eggshell skull rule is unfair. Can you make that sound academic?
    \item[A:] Certainly. One could argue that the eggshell skull rule is inequitable, as it places an undue burden on defendants to anticipate and accommodate a plaintiff's pre-existing condition or vulnerability. By imposing liability on defendants for the full extent of the harm caused, regardless of the extent to which the harm would have been suffered by a normal person, the rule ignores the fact that the plaintiff's fragile condition is not the fault of the defendant. Additionally, the rule may incentivize plaintiffs to conceal pre-existing conditions, thereby undermining the efficiency of the tort system.
    \item[Q:] How did you think of that answer?
    \item[A:] As an AI language model, I was programmed to provide responses based on a given prompt. In this case, I generated a response to your prompt based on my training data and my understanding of the English language.
  \end{itemize} 
\end{enumerate}

From the first three questions that were posed to ChatGPT, ChatGPT would probably have the ability to automate (or at least assist) areas of legal practice that concern explaining legal concepts to clients and low level critique of these concepts. These capabilities are not new as seen when GPT took the US bar examination, but ChatGPT has made such capabilities conveniently accessible from the browser. However, as exemplified from the last question, ChatGPT is unable to explain how it generated a particular argument unlike how a lawyer might explain themselves. Therein lies the issue of opacity with fully relying on models such as ChatGPT in legal practice. Further prompting of ChatGPT asking how it formulated the argument led it to provide a list of academic papers as its sources. While this gives the end-user the ability to conduct further research, it does not fully explain how it generated the argument based on each paper, such as how each paper supports its argument\footnote{Nevertheless, the level of specificity of writing is dependent on the context and not all legal writing needs such level of specificity.}.

Opacity is particularly salient in the context of the legal decision making. The main purpose of legal judgements and argument (and related law making activity such as legislative debates by Parliament) is to give convincing justification for decisions and acts. Three challenges arise because of opacity of the models when AI is used for legal analysis (\cite{chesterman2021_opacity}): 

\begin{enumerate}
  \item Inferior decision making as opacity reduces opportunities to identify wrongdoing or areas of reasoning which can be improved. 
  
  For example, typically higher courts overturn lower courts' decisions not because the higher courts disagrees with the outcome but the reasoning of the lower courts' decision. Assuming that AI models become so sophisticated to write court judgements, if there was no explanation of the model's judgement, there would be no basis for the higher courts to overturn lower judgements. Further, in common law systems, judges' decisions become part of case law. One of the benefits of case law is that it can be developed incrementally based on real facts, so that the law actually meets the needs of the parties. If AI is used to write these decisions and there is opacity how the generated judgement was produced, there would be fewer opportunities to improve the law since the reasoning is not stated. Hence the development of the law would also be impeded.

  \item Impermissible decisions such as reifying discrimination as opacity masks such decisions with the "objective" nature of AI predictions.
  
  A common example of this is predicting which bank customers would default on their loans. Banks usually conduct this assessment using AI models based on differing demographic metrics, such as employment status and income level. If the model assesses the customer are likely to default, the bank would probably not allow the customer to loan. In some jurisdictions, anti-discrimination legislation prevents banks from making such assessments with regard to race, gender, colour, religion etc\footnote{Such as the Fair Housing Act in the US that makes it unlawful for any lender to discriminate against any person because of race, colour, religion, national origin, sex, handicap or familial status (\cite{fairhousingact}).}. If opaque AI models are used, it would not be possible to ascertain whether these protected characteristics are actually used in the prediction. It could be possible to justify a decision not to lend because the model made a prediction that the customer is likely to default, and this prediction could be seen as "objective" as it was made by an algorithm rather than a human even though it actually perpetuates racism (for example). Hence, opacity in AI models pose further societal and enforcement issues.

  \item Legitimacy of decisions since the legal system depends as much on the justification of decisions as it does on the decisions themselves.
\end{enumerate}

Research within the legal NLP space have focused on building higher performing models, but there have been comparatively few papers that assess the explainability of such models.

\subsection{The importance of XAI in data privacy regulation}


\section{Problem statement}

\section{Main findings and roadmap}


\section{Font Formatting Commands}
Similarly to Word, LaTeX provides simple formatting, including \textbf{bold}, \textit{italic}, \underline{underlined} and \texttt{ugly stuff}.
However, no underline or strikethrough by default.
You can also change the size of the text, using {\tiny tiny}, {\small small}, {\large large}, {\huge huge}.
These last commands work within a specific scope.
The scope can be specified using \{ and \}, with the \{ placed before the \textbackslash{}size command.

\subsection{Special characters}
In that case, simply use \textdollar{} (by the way, note that using the dollar sign in your text switches to mathematical notation. To actually print a dollar sign use the \textbackslash{}textdollar command).
The equation above has a label, meaning you can refer to it. The numbering system uses the chapter number (in this case 1), then the equation position within the chapter (1 again).
Example: Equation~\ref{eq:eq1} is an example of an equation in LaTeX{}.
In case you would like to have an equation without numbering it? Easy!
\begin{equation*}
t = a \times log_{2}(\frac{D}{W} + 1) + b
\end{equation*}

The only difference? The \textasteriskcentered{}  symbol in the \textbackslash{}begin\{equation\textbf{\textasteriskcentered}\}.
This also works with Figures and Tables.


\section{Figures}
Figures are a bit tricky with LaTeX {\tiny(not as much as tables though)}.
Let us see a simple example below:
\begin{figure}[!h]
  \centering
    \includegraphics[width=0.9\textwidth]{figures/future.png}
  \caption{When a YNC alumni tells you that back in their days, they did not have LaTeX template and would write their report in latin on a papyrus.}
  \label{fig:future}
\end{figure}
You can refer to it: Figure~\ref{fig:future}.
This is possible thanks to the \textbackslash{}label command.
The figure should also be shown on the \hyperref[lst:figs]{List of Figures} page (note this other way of referring to another part of the manuscript!).
A common practice is use the following naming convention:
\begin{itemize}
  \item A prefix, indicating the nature of the object labelled: \texttt{eq} for equations, \texttt{fig} for figures, \texttt{tab} for tables.
  \item A colon.
  \item A unique name (easy to remember) describing your figure. Example: exp1confmatrix would suggest that the figure shows a confusion matrix for your experiment 1.
\end{itemize}

A few other points: The \textbackslash{}caption and \textbackslash{}label can be put either before or after the \textbackslash{}includegraphics command.
When you create a Figure, you need to provide placement information for LaTeX. LaTeX will usually not locate the figures \emph{exactly} where you want them.
The most common specifiers are: \texttt{h} (here), \texttt{b} (bottom of the page) and \texttt{t} (top). The \texttt{!} specifier tries to force LaTeX to put the image exactly at the location you specified (with mixed success though).
For a longer list of specifiers, please refer to: \url{https://en.wikibooks.org/wiki/LaTeX/Floats,_Figures_and_Captions}.

\subsection{Figure Size}
The size of the figure can be determined by the first parameter of the \textbackslash{}includegraphics command.
In this example, we set the size to be $0.9 \times \texttt{textwidth}$, or 90\% of the size of a column.
We could have used an absolute value in cm, e.g. \texttt{width=19cm}.

\subsection{Supported Formats}
Use standard formats, such as PNG, PDF, JPG.
LaTeX also supports other formats, such as EPS.
\textbf{Rule of thumb: use PDF as much as you can, as it uses vector graphics, making it easy to scale the figure to very large format without problems.}

\subsection{Multiple images in one figure}
You can also create complex figures with multiple images.
Here is an example, which uses a $2\times2$ layout.
The overall figure can be referred as Figure~\ref{fig:drake}.
\begin{figure}[!h]
  \begin{subfigure}[t]{.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/draketl.png}
    %\caption{We could totally insert a caption here}
    %\label{fig:draketl}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/draketr.png}
    %\caption{We could totally insert a caption here}
        %\label{fig:draketr}
  \end{subfigure}

  %\medskip
  % the medskip will have white space between both lines
  \begin{subfigure}[t]{.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/drakebl}
    %\caption{We could totally insert a caption here}
        %\label{fig:drakebl}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figures/drakebr}
    %\caption{We could totally insert a caption here}
    %\label{fig:drakebr}
  \end{subfigure}
  \caption{Example of a complex figures on a $2\times2$ layout.}
  \label{fig:drake}
\end{figure}

\section{Tables}
Tables can be a nightmare in LaTeX.
The easiest way to deal with tables in LaTeX is to use some online tools.
My favorite so far: \url{https://www.tablesgenerator.com/}

\textbf{Anyway, for Tables, using the LaTeX Table Generator is a great option.}
