\chapter{Conclusion} % Main chapter title
\label{chapter5} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\section{Findings}
We can now answer the research questions that were posed in Section~\ref{chap1:problem_statement}:
\begin{enumerate}
    \item Which classifiers perform the best on a data privacy dataset in terms of traditional performance metrics?
    
    Overall, SVC + Tf-IDF performed the best across the 5 data practices according to the weighted F1. This corresponds with the findings of Zimmeck et al. Particularly when classifying \texttt{Identifier Cookie 1st Party}, both SVC + Tf-IDF and logistic regression + Tf-IDF performed similarly with the same F1 score. It is surprising that GloVe embeddings performed worse compared to Tf-IDF, given that GloVe accounts for context as compared to Tf-IDF which is purely count-based.

    \item Which classifiers are the most explainable to users that include laypersons and users with domain knowledge in data science and the law?
    
    When SVC was compared to logistic regression, respondents by a large majority found no difference in the explainability. When Tf-IDF was compared to GloVe, respondents were undecided with an almost equal one-third split across the the three options (i.e. no difference, Tf-IDF \& GloVe). So just by looking at these votes, respondents did not find any difference in explainability when comparing SVC to logistic regression, but were undecided whether Tf-IDF or GloVe was more explainable. Comparing the respondents' votes with performance metrics, there seems to be a slight (but non-statistical) relationship between explainability and classifier performance. Tf-IDF consistently outperformed GloVe regardless of the model used, which is reflected in how respondents were equally split between the 3 options. Compared to logistic regression and SVC where latter did not consistently outperform the former, majority of respondents similarly found no difference between the explanations. 

    \item How would users rate the explainability of a selected XAI technique?
    
    There was a negative trend of respondents' self-reported scores of interpretability and understanding for LIME. Hence, one possible interpretation is that the individual explanations got less explainable for each question. Another interpretation is that global explainability decreased because respondents were exposed to contradictory information about the model with each new explanation they viewed, rather than local explainability being affected. In terms of predicting counterfactuals, it was not possible to perceive any consistent trend given that the classifiers' predicted probabilities for some classes were almost equal for some of the counterfactual sentences. 

    \item What are the differences in explainability if users were asked to consider the predictions of these classifiers from the perspective of a consumer, an organisation or the PDPC (as the regulatory authority for data privacy) after viewing the classifiers' explanations?
    
    Effectiveness and trust significantly decreased for context 2 (PDPC), and fairness for context 3 (consumer). Only effectiveness significantly increased for context 1 (app developer). This could suggest that respondents believe that as a regulatory body, PDPC should retain more discretion in making decisions instead of relying on AI, even though using these models could increase automation. Respondents believe that the risks of making a wrong decision outweighs the possible benefits to efficiency. Having realised the fallibility of the model, respondents have reduced trust as well. Respondents as consumers also seem to be only concerned about their own benefits that result from wrong / correct predictions of the model. Hence, it is not "fair" that they would not be compensated for violations to their data privacy just because the model can make wrong predictions. Respondents as app developers, unlike PDPC, believe that wrong predictions are an acceptable risk when analysing privacy policies can be automated without needing to pay for external legal advice.

    Further, explainability metrics overall significantly decreased for context 2 and 3, which could suggest that respondents felt the risks of making a wrong prediction because of the model's fallibility outweighed any efficiency, or cost gained from automation from the perspective of PDPC or a consumer. Trust also significantly decreased overall for all contexts. This could suggest that the model's fallibility influenced respondents' trust the most across the contexts, which points to a possible mismatch of expectations of what respondents thought the model could do and what was actually the case. 
\end{enumerate}


\section{Limitations and future work}
There are a few limitations:
\begin{enumerate}
    \item \textit{Limited number of respondents ($n = 31$), diversity of expertise and age}: The intent was to conduct a more in-depth cross sectional study of the self-reported scores of the respondents according to their area and depth of expertise (measured by their major or current occupation) and their beliefs relating to AI and data privacy. This analysis could be used to support certain inferences made earlier. For example, the inference was made that explainability metrics significantly decreased for the contexts of PDPC and consumer but not for the app developer because respondents believed that the app developer should have enough programming expertise to understand the classifier and does not need to rely only on the visualisations provided. To support this claim, a significance test could be used on the scores reported by respondents with data science background to check whether there was a significant decrease, and compared with those respondents without a data science background. However, with 31 respondents, breaking down the survey results into smaller sub-groups would not reliably show statistical trends\footnote{For example, Wilcoxon Rank Sum Test is recommended to be run on populations $> 20$.}. Hence, future work could be conducted on a much larger sample size to run these more in-depth analysis.
    
    \item \textit{Analysis is limited to the context of the APP-350 corpus, classifiers used, and LIME:} As mentioned above, assessing XAI is highly context dependent, and therefore the foregoing analysis should also be seen in light of these particular components. For example, a finding was that respondents found no difference in explainability between SVC and logistic regression. However, this finding is restricted to this particular dataset, as these models and LIME would probably process another sentence from another dataset differently. Future work to evaluate XAI within the context of data privacy could include using another data privacy dataset, or other LIME implementations with different styles of visualisation.
    
    \item \textit{Inherent drawbacks of LIME and human evaluation of XAI:} As LIME is a post-hoc local explainability technique, respondents' understanding of the model was limited to the example visualisations that were presented to them. The sentences and the order of these sentences shown to respondents were intended to show the limitations of the classifier. For example, the first two visualisations showed that "cookie" was the most important feature contributing to the classification, but the next two still classified the sentence as \texttt{Identifier Cookie 1st Party} even though "cookie" was not present. This paints a more confusing and inconsistent picture of the classifier's logic, as compared to four examples consistently showing that "cookie" was the most important feature. In fact, this was controlled for in another study where the authors only chose to show correctly classified sentences in order to reduce the information load on the respondents (\cite{gorski2021}). Hence, if more consistent examples were chosen instead, respondents might have reported differently. If post-hoc techniques are used, future work could include an interactive study where respondents can choose to generate their own sentences for classification, which allows them to test whether their understanding of the classifier's logic is consistent.
    
    Further, human evaluations of post-hoc explanations have been criticised for being inherently flawed because of confirmation bias. Post-hoc explanations may have little to no similarities to how the underlying classifier made the prediction as they are an oversimplification of the classifier's logic. Hence, human evaluation may have limited meaning (\cite{rosenfeld2021}). Instead, the authors propose four objective metrics that quantify the explanation itself and its appropriateness given the XAI goal. Future work could involve less reliance on human evaluation to incorporate more objective metrics, as well as choosing other types of XAI techniques such as global self explaining techniques.

    \item \textit{LinearSVC and logistic regression can be similar in their optimisation technique, which may influence explainability.}
\end{enumerate}

\section{Final comments}
(\cite{hacker2022varieties}) - about intersection of law and explainability in different areas of law

(\cite{yalcin2022perceptions}) - reason for why efficiency and trust is emphasised in this capstone

(\cite{vilone2021}) - about how there are so many different metrics as part of explainability, including efficiency, trust, transparency, understandability etc.

(\cite{kastner2021}) - about the relationship between trust and explainability - can be used to explain survey results

(\cite{waltl2018})- Possible to assign metrics of explainability to AI models (like a rating)