\chapter{Conclusion} % Main chapter title
\label{chapter5} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\section{Findings}
The research questions that were posed in Section~\ref{chap1:research_questions} can now be answered:
\begin{enumerate}
    \item \textbf{Which classifiers perform the best on a data privacy dataset in terms of traditional performance metrics?}
    
    Overall, SVC + Tf-IDF performed the best when measuring performance using weighted F1, though there was not much difference when comparing micro-averaged AUC for the ROC curves. In terms of performance when classifying \texttt{Identifier Cookie 1st Party}, both SVC + Tf-IDF and logistic regression + Tf-IDF performed similarly. It is surprising that GloVe embeddings performed worse overall and caused greater variance in the AUC of all the classes compared to Tf-IDF, given that GloVe accounts for semantic context as compared to Tf-IDF which is purely count-based.

    \item \textbf{Which classifiers are the most understandable to users that include laypersons and users with domain knowledge in data science and the law?}
    
    When SVC was compared to logistic regression, respondents by a large majority found no difference in the understandability. When Tf-IDF was compared to GloVe, respondents were undecided across the three options. Comparing the respondents' votes with performance metrics, there seems to be a slight (but non-statistical) relationship between understandability and classifier performance. Tf-IDF consistently outperformed GloVe regardless of the model used, which is reflected in how respondents were equally split between the 3 options. Whereas comparing logistic regression and SVC, the latter did not consistently outperform the former, and majority of respondents similarly found no difference between the explanations.

    \item \textbf{How would users rate the explainability of a selected XAI technique?}
    
    There was a statistically significant decrease in respondents' reported understandability but not interpretability of LIME, and there was a negative trend of both understandability and interpretability across the questions. One interpretation is that global rather than local explainability decreased because respondents were exposed to contradictory information about the model with each new explanation they viewed.

    \item \textbf{What are the differences in explainability if users were asked to consider the predictions of these classifiers from the perspective of a consumer, an organisation and the PDPC?}
    
    Effectiveness and trust significantly decreased for the PDPC context, and fairness for the consumer context. Only effectiveness significantly increased for the app developer context. Explainability metrics in general significantly decreased specifically for the PDPC and consumer context, and overall trust across all three contexts also significantly decreased. My inferences of these results were:
    \begin{enumerate}
        \item Efficiency is dependent on the risks of making wrong predictions,
        \item Classifiers' interpretability is especially critical for the PDPC such that trust was affected,
        \item Fairness is perceived in relation to the user's own interests as opposed to systemic fairness,
        \item Expertise of the end user of the explanations affects the explainability metrics; and
        \item Mismatch of expectations particularly affects trust.
    \end{enumerate}
\end{enumerate}

\subsection{Evaluation of survey design}
As mentioned in Section~\ref{chap1:research_questions}, this capstone also serves as a trial for XAI human evaluation that accounts for the different purposes of explanations. Overall, I would consider this survey design a success, given the statistically significant results reported. Nevertheless, I note a few points of improvement:

\begin{enumerate}
    \item \textit{Possible information overload:} Given the technical nature of XAI in addition to the legal context of the PDPA, a level of exposition was unavoidable as the survey was targeted at a general audience. In particular, though the three contexts could have been shorter and simpler, some level of detail was still necessary to establish the different purposes of explanation within a realistic scenario. Some respondents, especially those without any background in AI or law, later provided feedback that the survey was a challenge to complete because of the complexity of the background information needed to fully understand the contexts. Also, as I relied mostly on textual expositions, the linguistic ability of respondents could also have affected their ability to understand the background concepts. Therefore, future human evaluation should be mindful of such information overload and reliance on one medium of communication. Other forms of media could be used such as a short video primer of the background information. Alternatively, instead of showing all the contexts to a general audience, respondents of the relevant background could be shown specific contexts and explanations that are related to their particular expertise. However, the analysis of how explainability metrics change across the different contexts would then not be possible. More generally, this speaks to how technical current XAI is, such that the average person is probably unlikely to understand the visualisations without background information.
    
    \item \textit{Incorporation of qualitative evaluation:} I refrained from asking the respondents too many open-ended questions\footnote{In Part 3 of the survey, I asked respondents to explain why they provided the score they did of the understandability of the visualisation. However, due to time constraints, I did not use these qualitative responses in the analysis.} to limit the duration of the survey. However, qualitative evaluation could be used to support certain inferences made above, especially in relation to the evaluation of the changes in explainability metrics since these changes involve the interaction of different values with different purposes of the explanations. Further studies that investigate beliefs of respondents should consider supporting quantitative analyses with qualitative data.

    \item \textit{Possible interaction between model architecture and reported survey scores:} Though Part 4 \& Part 5 of the survey attempted to investigate the relationship between objective classifier performance metrics with the subjective perceptions of understandability of respondents, these performance metrics were intended to be merely a proxy for investigating whether some classifiers are inherently more understandable because of their architecture. Due to limited time and expertise, I did not consider whether there such was a relation. Therefore, further work could investigate the viability of assigning objective explainability ratings to AI models based on their inherent architecture \cite{waltl2018}, similar to car safety ratings. 
\end{enumerate}

\section{Limitations and future work}
There are a few limitations:
\begin{enumerate}
    \item \textit{Limited number of respondents ($n = 31$), diversity of expertise and age}: The original intent was to conduct a more in-depth cross-sectional study of the self-reported scores of the respondents according to their area of expertise and their beliefs relating to AI and data privacy. This analysis could be used to support certain inferences made earlier such as explainability is dependent on expertise of the end user. However, with 31 respondents, breaking down the survey results into smaller sub-groups would not reliably show statistical trends\footnote{For example, Wilcoxon Rank Sum Test is recommended to be run on populations $> 20$.}. Hence, future work could be conducted on a much larger sample size to run more cross-sectional analysis.
    
    \item \textit{Analysis is limited to the context of the APP-350 corpus, classifiers used, and LIME:} Assessing XAI is highly context dependent, and therefore the foregoing analysis should also be seen in light of these particular components. Future work to evaluate XAI within the context of data privacy could include using another data privacy dataset, other classifiers and other XAI techniques apart from LIME. In particular, LIME may interact differently with current state-of-the-art classifiers which are much more complex and rely on deep learning given that NLP has developed quite substantially since 2016 when LIME was first introduced.
    
    \item \textit{Inherent drawbacks of LIME and human evaluation of XAI:} As LIME is a post-hoc local explainability technique, respondents' understanding of the model was limited to the example visualisations that were presented to them. The sentences shown to respondents were intended to show the limitations of the classifier. This paints a more confusing and inconsistent picture of the classifier's logic, as compared to examples that consistently show "cookie" was the most important feature. In fact, this was controlled for in another study where the authors only chose to show correctly classified sentences in order to reduce the information load on the respondents \cite{gorski2021}. Hence, if more consistent examples were chosen instead, respondents might have responded differently. If post-hoc techniques are used, future work could include an interactive dashboard where respondents can choose to generate their own sentences for classification, which allows them to test their understanding of the classifier's logic is consistent.
    
    Further, human evaluations of post-hoc explanations have been criticised for being inherently flawed because of confirmation bias. Post-hoc explanations may have little to no similarities to how the underlying classifier made the prediction as they are an oversimplification of the classifier's logic. Hence, human evaluation may have limited meaning \cite{rosenfeld2021}. Instead, the authors propose four objective metrics that quantify the explanation itself and its appropriateness given the XAI goal. Future work could involve less reliance on human evaluation to incorporate more objective metrics, as well as choosing other types of XAI techniques such as global self explaining techniques. 

    \item \textit{Investigation of the differences in interaction between explainability metrics and other areas of algorithmic legal decision making:} The explainability metrics used in Section~\ref{sec:three_contexts_comparison} were presumed to be important values for a legal context because of their link to the rule of law. Nevertheless, there are other values that could be relevant such as cost because of access to justice and predictability because the legal system should provide consistent rules for its subjects to follow. Further investigation could also be conducted on how these values change and affect explainability in different areas (i.e. tort vs data privacy) and fora (i.e. PDPC vs the Court of Appeal) of the law because of different legal standards \cite{hacker2022varieties} and the expectations \& pre-conceptions of users of the legal system \cite{yalcin2022perceptions}.
    
    In particular, as the intuition behind increasing explainability is to increase trust of users, it is worth further investigating this relationship between explainability and trust. There has been evidence that shows this relation is not as general as previously thought \cite{kastner2021}. In fact, if explanations reveal problems about the system, trust could decrease rather than increase, which seems to be the case in this capstone where respondents were intentionally given examples that demonstrated the limitations of the classifiers.
\end{enumerate}

\section{Final thoughts}
This capstone has barely scratched the surface of the broader issues of XAI and the law which include defining legal standards for explainability and translating them into technical XAI requirements. To this end, the AI Act proposed by the European Commission differentiates AI systems into those which are "high risk" and "medium risk", with "high risk" systems requiring a greater level of explainability that is user-empowering and compliance-oriented \cite{sovrano2022metrics}. The Act is also likely to have greater scope than the GDPR as the Act is not limited to personal data \cite{lilian_ai_act}. Nevertheless, discussions for the Act started in April 2021 and are expected to end by late 2023. In comparison, OpenAI took less than 4 months from scoring around 45\% on the US Bar Exam using GPT-3 to coming in the 90th percentile by GPT-4 \cite{katz2023gpt}. I eagerly await the debates that follow.