\chapter{Conclusion} % Main chapter title
\label{chapter5} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

In summary, this capstone has conducted an uniquely empirical evaluation of the XAI of NLP within a data privacy context from two perspectives: The first through traditional performance metrics of machine learning, and the second through human evaluation. The survey investigated how explainability interacted with values that are particularly important to a legal domain, and tested the effectiveness of LIME through different types of questions posed to respondents such as counterfactual reasoning.

\section{Findings}
The research questions that were posed in Section~\ref{chap1:research_questions} can now be answered:
\begin{enumerate}
    \item \textbf{Which classifiers perform the best on a data privacy dataset in terms of traditional performance metrics?}
    
    Overall, SVC + Tf-IDF performed the best when measuring performance using weighted F1, though there was not much difference when comparing micro-averaged AUC for the ROC curves. In terms of performance when classifying \texttt{Identifier Cookie 1st Party}, both SVC + Tf-IDF and logistic regression + Tf-IDF performed similarly. It is surprising that GloVe embeddings performed worse overall and caused greater variance in the AUC of all the classes compared to Tf-IDF, given that GloVe accounts for semantic context as compared to Tf-IDF which is purely count-based.

    \item \textbf{Which classifiers are the most understandable to users that include laypersons and users with domain knowledge in data science and the law?}
    
    When SVC was compared to logistic regression, respondents by a large majority found no difference in the understandability. When Tf-IDF was compared to GloVe, respondents were undecided across the three options. Comparing the respondents' votes with performance metrics, there seems to be a slight (but non-statistical) relationship between understandability and classifier performance. Tf-IDF consistently outperformed GloVe regardless of the model used, which is reflected in how respondents were equally split between the 3 options. Whereas comparing logistic regression and SVC, the latter did not consistently outperform the former, and majority of respondents similarly found no difference between the explanations.

    \item \textbf{How would users rate the explainability of a selected XAI technique?}
    
    There was a statistically significant decrease in respondents' reported understandability but not interpretability of LIME, and there was a negative trend of both understandability and interpretability across the questions. One interpretation is that global rather than local explainability decreased because respondents were exposed to contradictory information about the model with each new explanation they viewed.

    \item \textbf{What are the differences in explainability if users were asked to consider the predictions of these classifiers from the perspective of a consumer, an organisation or the PDPC?}
    
    Effectiveness and trust significantly decreased for the PDPC context, and fairness for the consumer context. Only effectiveness significantly increased for the app developer context. Explainability metrics significantly decreased for PDPC and consumer context, and overall trust also significantly decreased. Some inferences of these results were:
    \begin{enumerate}
        \item Efficiency is dependent on the risks of making wrong predictions,
        \item Classifiers' interpretability is especially critical for the PDPC such that trust was affected,
        \item Fairness is perceived in relation to the user's own interests as opposed to systemic fairness,
        \item Expertise of the end user of the explanations affects explainability; and
        \item Mismatch of expectations particularly affects trust.
    \end{enumerate}
\end{enumerate}

\section{Limitations and future work}
There are a few limitations:
\begin{enumerate}
    \item \textit{Limited number of respondents ($n = 31$), diversity of expertise and age}: The original intent was to conduct a more in-depth cross-sectional study of the self-reported scores of the respondents according to their area of expertise and their beliefs relating to AI and data privacy. This analysis could be used to support certain inferences made earlier that explainability is dependent on expertise of the end user. However, with 31 respondents, breaking down the survey results into smaller sub-groups would not reliably show statistical trends\footnote{For example, Wilcoxon Rank Sum Test is recommended to be run on populations $> 20$.}. Hence, future work could be conducted on a much larger sample size to run more cross-sectional analysis.
    
    \item \textit{Analysis is limited to the context of the APP-350 corpus, classifiers used, and LIME:} Assessing XAI is highly context dependent, and therefore the foregoing analysis should also be seen in light of these particular components. Future work to evaluate XAI within the context of data privacy could include using another data privacy dataset, other classifiers and other XAI techniques apart from LIME. In particular, LIME may interact differently with current state-of-the-art classifiers which are much more complex and rely on deep learning given that NLP has developed quite substantially since 2016 when LIME was first introduced.
    
    \item \textit{Inherent drawbacks of LIME and human evaluation of XAI:} As LIME is a post-hoc local explainability technique, respondents' understanding of the model was limited to the example visualisations that were presented to them. The sentences shown to respondents were intended to show the limitations of the classifier. This paints a more confusing and inconsistent picture of the classifier's logic, as compared to examples that consistently show "cookie" was the most important feature. In fact, this was controlled for in another study where the authors only chose to show correctly classified sentences in order to reduce the information load on the respondents \cite{gorski2021}. Hence, if more consistent examples were chosen instead, respondents might have responded differently. If post-hoc techniques are used, future work could include an interactive dashboard where respondents can choose to generate their own sentences for classification, which allows them to test their understanding of the classifier's logic is consistent.
    
    Further, human evaluations of post-hoc explanations have been criticised for being inherently flawed because of confirmation bias. Post-hoc explanations may have little to no similarities to how the underlying classifier made the prediction as they are an oversimplification of the classifier's logic. Hence, human evaluation may have limited meaning \cite{rosenfeld2021}. Instead, the authors propose four objective metrics that quantify the explanation itself and its appropriateness given the XAI goal. Future work could involve less reliance on human evaluation to incorporate more objective metrics, as well as choosing other types of XAI techniques such as global self explaining techniques. Future work could also investigate assigning objective explainability ratings to AI models \cite{waltl2018}, similar to car safety ratings.

    \item \textit{Investigation of the differences in interaction between explainability metrics and other areas of algorithmic legal decision making:} The explainability metrics used in Section~\ref{sec:three_contexts_comparison} were presumed to be important values for a legal context because of their link to the rule of law. Nevertheless, there are other values that could be relevant such as cost because of access to justice and predictability because the legal system should provide consistent rules for its subjects to follow. Further investigation could also be conducted on how these values change and affect explainability in different areas (i.e. tort vs data privacy) and fora (i.e. PDPC vs the Court of Appeal) of the law because of different legal standards \cite{hacker2022varieties} and the expectations \& pre-conceptions of users of the legal system \cite{yalcin2022perceptions}.
    
    In particular, as the intuition behind increasing explainability is to increase trust of users, it is worth further investigating this relationship between explainability and trust. There has been evidence that shows this relation is not as general as previously thought \cite{kastner2021}. In fact, if explanations reveal problems about the system, trust could decrease rather than increase, which seems to be the case in this capstone where respondents were intentionally given examples that demonstrated the limitations of the classifiers.
\end{enumerate}

\section{Final comments}
Ultimately, the application of XAI and the law is a multi-disciplinary field that requires both empirical and normative investigation from the different disciplines of data science, law, psychology, philosophy, social sciences and more. This capstone only scratches the surface of the broader issues of XAI and the law, including defining legal standards for explainability and translating them into technical XAI requirements. To this end, the AI Act proposed by the European Commission differentiates AI systems into those which are "high risk" and "medium risk", with "high risk" systems requiring a greater level of explainability that is user-empowering and compliance-oriented \cite{sovrano2022metrics}, though the details and implementation are still under discussion.

Thus, even the law is not exempt from the winds of change despite its long tradition and established processes. With suggestions that GPT-4 has the markings of artificial general intelligence \cite{bubeck2023sparks}, and Google's \$400 million investment in AI firm Anthropic in response to Microsoft's \$10 billion investment into OpenAI \cite{bloomberg_google}, algorithmic decision making as a whole is not only going to be more pervasive, but also likely more opaque. Law, as an institution centered on explanation and justification through argumentation, should also be equally concerned with the advancement of XAI.