\chapter{Conclusion} % Main chapter title
\label{chapter5} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\section{Findings}
The research questions that were \hyperref[chap1:research_questions]{posed} can now be answered:
\begin{enumerate}
    \item \textbf{Which classifiers perform the best on a data privacy dataset in terms of traditional performance metrics?}
    
    Across all classes, SVC + Tf-IDF performed the best when measuring performance using weighted F1, though there was not much difference when comparing micro-averaged AUC for the ROC curves. In terms of performance specifically for \texttt{Identifier Cookie 1st Party}, both SVC + Tf-IDF and logistic regression + Tf-IDF performed similarly. It is surprising that GloVe embeddings performed worse overall and caused greater variance in the AUC of all the classes compared to Tf-IDF, given that GloVe accounts for semantic context as compared to Tf-IDF which is purely count-based.

    \item \textbf{Which classifiers are the most understandable to users that include laypersons and users with domain knowledge in data science and the law?}
    
    When SVC was compared with logistic regression, respondents by a large majority found no difference in their understandability. When Tf-IDF was compared to GloVe, respondents were undecided across the three options. Comparing respondents' votes with performance metrics, there seems to be a slight (but non-statistical) relationship between understandability and classifier performance. Tf-IDF consistently outperformed GloVe regardless of the model used, which is somewhat reflected by how respondents were overall undecided between the three options. In contrast, when comparing the performance of logistic regression and SVC, the latter did not consistently outperform the former, and majority of respondents similarly found no difference between the explanations.

    \item \textbf{How would users rate the explainability of a selected XAI technique?}
    
    There was a statistically significant decrease in respondents' reported understandability but not interpretability of LIME, and there was a negative trend of both understandability and interpretability across the questions. One interpretation is that global rather than local explainability decreased because respondents were exposed to contradictory information about the classifier with each new explanation they viewed.

    \item \textbf{What are the differences in explainability if users were asked to consider the predictions of these classifiers from the perspective of a consumer, an organisation and the PDPC?}
    
    Effectiveness and trust significantly decreased for the PDPC context, and fairness for the consumer context. Only effectiveness significantly increased for the app developer context. Explainability metrics in general significantly decreased specifically for the PDPC and consumer context, and overall trust when taking the mean scores across the contexts also significantly decreased.
\end{enumerate}

Overall, these results confirm that explainability and values related to explainability are dependent on the purposes of explanation. Some values are more related to explainability in certain contexts. This may not be surprising since intuitively, if AI models substitute decision-making previously done by humans, the models' reasoning process would naturally be compared with the standards of human decision-making. For example, AI that substitutes the decision-making of a secretary to propose mutually convenient meeting times would be expected to prioritise efficiency over transparency. Similarly, algorithmic legal decision-making would have its own specific values and prioritisation of these values which I hope the preceding analysis has brought some insight to.

Additionally, there is some suggestion that higher performing classifiers could be more understandable. The performance metrics were intended to be a proxy for the classifiers' architecture. Therefore, further work could investigate the viability of assigning more objective explainability ratings to AI models based on their inherent architecture \cite{waltl2018}, similar to car safety ratings.

\section{Limitations and future work}
There are a few limitations:
\begin{enumerate}[listparindent=0.5cm]
    \item \textit{Limited number of respondents ($n = 31$), diversity of expertise and age}: The original intent was to conduct a more in-depth cross-sectional study of the self-reported scores of the respondents according to their area of expertise and their beliefs relating to AI and data privacy. This analysis could be used to support certain inferences made earlier such as explainability is dependent on expertise of the end user. However, with 31 respondents, breaking down the survey results into smaller sub-groups would not reliably show statistical trends\footnote{For example, Wilcoxon Rank Sum Test is recommended to be run on populations $> 20$.}. Hence, future work could be conducted on a much larger sample size to run cross-sectional analysis.
    
    \item \textit{Analysis is limited to the context of the APP-350 corpus, classifiers used, and LIME:} Assessing XAI is highly context dependent, and therefore the foregoing analysis should also be seen in light of these particular components. Future work to evaluate XAI within the context of data privacy could include using another data privacy dataset, other classifiers and other XAI techniques apart from LIME. In particular, LIME may interact differently with current state-of-the-art classifiers which are much more complex and rely on deep learning given that NLP has developed quite substantially since 2016 when LIME was first introduced.
    
    \item \textit{Inherent drawbacks of LIME and human evaluation of XAI:} As LIME is a post-hoc local explainability technique, respondents' understanding of the model was limited to the example visualisations that were presented to them. The sentences shown to respondents were intended to show the limitations of the classifier. This paints a more confusing and inconsistent picture of the classifier's logic, as compared to examples that consistently show "cookie" was the most important feature. In fact, this was implemented in another study where the authors only chose to show correctly classified sentences in order to reduce the information load on the respondents \cite{gorski2021}. If local techniques are used, future work could include an interactive dashboard where respondents can choose to generate their own sentences for classification, which allows them to test their understanding of the classifier's logic.
    
    Further, human evaluations of post-hoc explanations have been criticised for being inherently flawed because of confirmation bias and therefore such evaluations may have limited meaning \cite{rosenfeld2021}. Instead, \cite{rosenfeld2021} propose four objective metrics that quantify the explanation itself and its appropriateness given the XAI goal. Future work could involve less reliance on human evaluation to incorporate more objective metrics, as well as choosing other types of XAI techniques such as global self explaining techniques. 

    \item \textit{Differences in interaction between explainability metrics and other areas of algorithmic legal decision making:} The explainability metrics \hyperref[sec:three_contexts_comparison]{used} were presumed to be important values for a data privacy context because of their link to the rule of law. Nevertheless, there are other values that could be relevant such as cost because of access to justice and predictability because the legal system should provide consistent rules for people to follow. Further investigation could be conducted on how these values change and affect explainability in different areas (i.e. tort vs data privacy) and fora (i.e. PDPC vs the Court of Appeal) of the law because of different legal standards \cite{hacker2022varieties} and the expectations \& pre-conceptions of users of the legal system \cite{yalcin2022perceptions}.
\end{enumerate}

\subsection{Evaluation of survey design}
As mentioned in the description of \hyperref[sec:survey_method]{survey design}, this capstone also serves as a trial for XAI human evaluation that assesses explainability across different selected purposes of explanations. Overall, I would consider this survey design a success, given the statistically significant results reported. Nevertheless, I note two points of improvement.

First, there could be possible information overload. Some respondents, especially those without any background in AI or law, provided feedback that the survey was a challenge to complete because of the complexity of concepts. Also, as I relied mostly on textual expositions, the linguistic ability of respondents could also have affected their ability to understand the questions. Therefore, future human evaluation should be mindful of such information overload and reliance on text. This highlights the technicality of XAI, making it difficult for the average person to understand the visualizations without prior knowledge.
    
Second, surveys could incorporate more qualitative evaluation. Since I allowed respondents to use whichever definition of the explainability metric they found most appropriate, this inevitably introduced certain assumptions and ambiguity in the interpretation of the results. Hence, qualitative evaluation could be used to support certain inferences made above.

\section{Final thoughts}
This capstone has barely scratched the surface of the broader issues of XAI and the law which include defining legal standards for explainability and translating them into technical XAI requirements. To this end, the AI Act is the largest of such endeavours as it has greater scope than the GDPR since it is not limited to personal data \cite{lilian_ai_act}. Nevertheless, discussions for the Act started in April 2021 and are expected to end by late 2023. In comparison, OpenAI took less than 4 months from scoring around 45\% on the US Bar Exam using GPT-3 to coming in the 90th percentile by GPT-4 \cite{katz2023gpt}. I eagerly await the debates that follow.